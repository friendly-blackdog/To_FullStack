# Data Analysis
# Contents

### 0) Introduction 
문제를 정의, 분해, 평가, 결정하는 일련의 workflow에 대해 설명합니다. 

<img width="363" alt="DataAnalysis" src="https://user-images.githubusercontent.com/39859458/73353664-2f77be80-42d7-11ea-962e-11f167176782.png">

```
출처: https://twitter.com/pollicyorg/status/1105783656499634176
```

Workflow에 대해 적당한 예제가 있다면, 되도록이면 예제를 들어 일련의 DataAnalysis를 설명하며 각 단계별로 설명할 예정입니다. 
- 문제를 작게 분해해서 생각하고,
- 가설을 세웁니다. 적절한 가설을 세우기 위해 이론을 검증하고
- 최적화 한뒤
- 데이터 시각화
- 가설을 검정합니다. 
- 베이지안 통계를 활용하여 
- 주관적 확률
- 히스토그램
- 회귀 분석
- 오차 분석을 통한 검증
- DB를 통한 Data 관리

---------------------------------------------------------------------------------------------

#### 위 0번의 예제를 기반으로 1)~7)은 실제 Data를 분석하는데 사용하는 Data Wrangling기법에 대해 주로 다룹니다. 
#### 즉, Data ETL(Extract, Transform, Load)에 대한 내용이 주 입니다. 

|변경전 목차| 변경 후 목차|
|:--------|:--------:|
| Numpy |NumPy 사용법. Data를 행렬로 표현해 보자, Pandas 사용법. Data를 Table로 표현해 보자|
| Pandas |-| 
| Visualization |Matplotlib,Seaborn,D3.js사용법, Data를 시각화 해보자 |
| CSV, JSON, XML |여러가지 파일 포맷 다루기 (CSV, JSON, XML, Excel, PDF) | 
| 파일 다루기(file, glob, os, sys) | 파일 조작에 유용한 모듈과 파일 체계화 하기(file, glob, os, sys // shutle, zipfile) |
| 파일 체계화(Shutle, zipfile) | DataBase로 파일 관리하기 (DB, MySQL다루기) | 
| WebScrap(BeautifulSoup, Request, Selenium) | Web에서 Data긁어오기(WebScrap, BeautifulSoup, Request, Selenium) |
| DataBase활용하기, MongoDB | BigData 다루기0 (Bigdata와 관련 환경설정소개 (도커를 이용한 하둡과 스파크 환경설정) | 
| - | BigData다루기 1 (Spark를 활용하여 data pipeline 설계)| 
| - | BigData다루기 2 (하둡, 맵리듀스를 활용하한 data pipeline설계) | 


### 1) Numpy
### 2) Pandas
### 3) Visualsization
sample: https://github.com/YunaSon/Sample/blob/master/DataAnalysis/3.DataVisualization.ipynb
### 4) CSV, JSON, XML
### 5) Re (정규표현식)
### 6) 파일 다루기: file, glob, os, sys
### 6) 파일 체계화: Shutle, zipfile, 
### 7) WebScrap: BeautifulSoup, Request, Selenium
### 8) DataBase활용하기 : MongoDB
